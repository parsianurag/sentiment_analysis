{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:/Guvi/Own_Projects/Sentiment_Analysis_on_Social_Media\\\\preprocessed_data.pkl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Load your dataset\n",
    "file_path = r'D:/Guvi/Own_Projects/Sentiment_Analysis_on_Social_Media/twitter_training.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Rename columns for clarity\n",
    "df.columns = ['ID', 'Category', 'Sentiment', 'Text']\n",
    "\n",
    "# Drop any rows where 'Text' is null\n",
    "df_cleaned = df.dropna(subset=['Text'])\n",
    "\n",
    "# Drop duplicates\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['Text'])\n",
    "\n",
    "# Define manual stopwords\n",
    "manual_stopwords = {\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your',\n",
    "    'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it',\n",
    "    \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this',\n",
    "    'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
    "    'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "    'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n",
    "    'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\n",
    "    'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most',\n",
    "    'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can',\n",
    "    'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren',\n",
    "    \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "    \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\",\n",
    "    'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n",
    "}\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters\n",
    "    text = ' '.join(word for word in text.split() if word not in manual_stopwords)  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "df_cleaned['Cleaned_Text'] = df_cleaned['Text'].apply(clean_text)\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Vectorize the cleaned text\n",
    "X = vectorizer.fit_transform(df_cleaned['Cleaned_Text']).toarray()\n",
    "\n",
    "# Encode the sentiment labels (assuming 'Positive' is 1 and others are 0)\n",
    "y = df_cleaned['Sentiment'].apply(lambda x: 1 if x == 'Positive' else 0)\n",
    "\n",
    "# Save the vectorizer for future use in deployment\n",
    "joblib.dump(vectorizer, os.path.join(os.path.dirname(file_path), 'tfidf_vectorizer.pkl'))\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the preprocessed data with compression to save space\n",
    "joblib.dump((X_train, X_test, y_train, y_test), os.path.join(os.path.dirname(file_path), 'preprocessed_data.pkl'), compress=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7337026910346812\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84     10076\n",
      "           1       0.98      0.03      0.06      3822\n",
      "\n",
      "    accuracy                           0.73     13898\n",
      "   macro avg       0.85      0.52      0.45     13898\n",
      "weighted avg       0.80      0.73      0.63     13898\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['D:/Guvi/Own_Projects/Sentiment_Analysis_on_Social_Media\\\\sentiment_rf_model_optimized.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load preprocessed data\n",
    "file_path = r'D:/Guvi/Own_Projects/Sentiment_Analysis_on_Social_Media/preprocessed_data.pkl'\n",
    "X_train, X_test, y_train, y_test = joblib.load(file_path)\n",
    "\n",
    "# If memory issues persist, you can use a smaller subset of the training data\n",
    "# X_train, _, y_train, _ = train_test_split(X_train, y_train, train_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest model with optimized parameters\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=50,            # Reduce number of trees\n",
    "    max_depth=20,               # Limit the depth of trees\n",
    "    min_samples_split=10,       # Minimum samples required to split an internal node\n",
    "    n_jobs=-1,                  # Utilize all available cores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Classification Report:\\n{report}\")\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = os.path.join(os.path.dirname(file_path), 'sentiment_rf_model_optimized.pkl')\n",
    "joblib.dump(model, model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1) Data Preprocessing :                                                                                                                    import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Load your dataset\n",
    "file_path = r'D:/Guvi/Own_Projects/Sentiment_Analysis_on_Social_Media/twitter_training.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Rename columns for clarity\n",
    "df.columns = ['ID', 'Category', 'Sentiment', 'Text']\n",
    "\n",
    "# Drop any rows where 'Text' is null\n",
    "df_cleaned = df.dropna(subset=['Text'])\n",
    "\n",
    "# Drop duplicates\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['Text'])\n",
    "\n",
    "# Define manual stopwords\n",
    "manual_stopwords = {\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your',\n",
    "    'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it',\n",
    "    \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this',\n",
    "    'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
    "    'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "    'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n",
    "    'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\n",
    "    'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most',\n",
    "    'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can',\n",
    "    'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren',\n",
    "    \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "    \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\",\n",
    "    'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n",
    "}\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters\n",
    "    text = ' '.join(word for word in text.split() if word not in manual_stopwords)  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "df_cleaned['Cleaned_Text'] = df_cleaned['Text'].apply(clean_text)\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Vectorize the cleaned text\n",
    "X = vectorizer.fit_transform(df_cleaned['Cleaned_Text']).toarray()\n",
    "\n",
    "# Encode the sentiment labels (assuming 'Positive' is 1 and others are 0)\n",
    "y = df_cleaned['Sentiment'].apply(lambda x: 1 if x == 'Positive' else 0)\n",
    "\n",
    "# Save the vectorizer for future use in deployment\n",
    "joblib.dump(vectorizer, os.path.join(os.path.dirname(file_path), 'tfidf_vectorizer.pkl'))\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the preprocessed data with compression to save space\n",
    "joblib.dump((X_train, X_test, y_train, y_test), os.path.join(os.path.dirname(file_path), 'preprocessed_data.pkl'), compress=3)                                                                                           2) model training :                                                                                                                  import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import os\n",
    "\n",
    "# Load preprocessed data\n",
    "file_path = r'D:/Guvi/Own_Projects/Sentiment_Analysis_on_Social_Media/preprocessed_data.pkl'\n",
    "X_train, X_test, y_train, y_test = joblib.load(file_path)\n",
    "\n",
    "# Feature selection using Random Forest's feature importance\n",
    "feature_selector = SelectFromModel(RandomForestClassifier(n_estimators=50, random_state=42))\n",
    "X_train_selected = feature_selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = feature_selector.transform(X_test)\n",
    "\n",
    "# Initialize the Random Forest model with optimized parameters\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=40,            # Reduce number of trees slightly\n",
    "    max_depth=25,               # Slightly increase the depth\n",
    "    min_samples_split=10,       # Minimum samples required to split an internal node\n",
    "    min_samples_leaf=5,         # Increase min samples per leaf to prevent overfitting\n",
    "    max_features='sqrt',        # Use the square root of the number of features\n",
    "    n_jobs=-1,                  # Utilize all available cores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Stratified K-Fold Cross-Validation for better generalization (optional)\n",
    " skf = StratifiedKFold(n_splits=3)\n",
    " for train_index, val_index in skf.split(X_train_selected, y_train):\n",
    "     X_train_fold, X_val_fold = X_train_selected[train_index], X_train_selected[val_index]\n",
    "     y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "     model.fit(X_train_fold, y_train_fold)\n",
    "     val_pred = model.predict(X_val_fold)\n",
    "     print(f\"Validation Accuracy: {accuracy_score(y_val_fold, val_pred)}\")\n",
    "\n",
    "# Train the model on the full training data\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Classification Report:\\n{report}\")\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = os.path.join(os.path.dirname(file_path), 'sentiment_rf_model_optimized.pkl')\n",
    "joblib.dump(model, model_save_path)\n",
    " 3) deployment Stream lit app :                                                                                                     import streamlit as st\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters\n",
    "    text = ' '.join(word for word in text.split() if word not in manual_stopwords)  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Load the TF-IDF vectorizer and the trained model\n",
    "dataset_path = r'D:/Guvi/Own_Projects/Sentiment_Analysis_on_Social_Media/twitter_training.csv'\n",
    "data_directory = os.path.dirname(dataset_path)\n",
    "\n",
    "vectorizer_path = os.path.join(data_directory, 'tfidf_vectorizer.pkl')\n",
    "model_path = os.path.join(data_directory, 'sentiment_model.pkl')\n",
    "\n",
    "vectorizer = joblib.load(vectorizer_path)\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "# Define manual stopwords\n",
    "manual_stopwords = {\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your',\n",
    "    'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it',\n",
    "    \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this',\n",
    "    'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
    "    'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "    'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n",
    "    'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\n",
    "    'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most',\n",
    "    'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can',\n",
    "    'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren',\n",
    "    \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "    \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\",\n",
    "    'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n",
    "}\n",
    "\n",
    "# Streamlit app layout\n",
    "st.set_page_config(page_title=\"Sentiment Analysis\", page_icon=\":sparkles:\", layout=\"wide\")\n",
    "\n",
    "# CSS for styling\n",
    "st.markdown(\"\"\"\n",
    "    <style>\n",
    "        .main {\n",
    "            color: #333;\n",
    "            background-color: #f5f5f5;\n",
    "            padding: 20px;\n",
    "            border-radius: 10px;\n",
    "            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n",
    "        }\n",
    "        .input-box {\n",
    "            background-color: #ffffff;\n",
    "            border: 1px solid #ddd;\n",
    "            border-radius: 8px;\n",
    "            padding: 10px;\n",
    "            margin-bottom: 20px;\n",
    "        }\n",
    "        .text-output {\n",
    "            padding: 15px;\n",
    "            border-radius: 8px;\n",
    "            color: #ffffff;\n",
    "        }\n",
    "        .positive {\n",
    "            background-color: #4CAF50;\n",
    "        }\n",
    "        .negative {\n",
    "            background-color: #F44336;\n",
    "        }\n",
    "        .stButton > button {\n",
    "            background-color: #007BFF;\n",
    "            color: white;\n",
    "            border-radius: 8px;\n",
    "            padding: 10px;\n",
    "            border: none;\n",
    "        }\n",
    "        .stButton > button:hover {\n",
    "            background-color: #0056b3;\n",
    "        }\n",
    "    </style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Title and description\n",
    "st.markdown(\"<div class='main'><h1 style='text-align:center;'>üó£Ô∏è Sentiment Analysis Tool</h1></div>\", unsafe_allow_html=True)\n",
    "st.markdown(\"\"\"\n",
    "    <div class='main'>\n",
    "    <p style='text-align:center;'>Welcome to the **Sentiment Analysis Tool**! üéâ</p>\n",
    "    <p style='text-align:center;'>Enter your text below to get sentiment analysis results.</p>\n",
    "    </div>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Text input\n",
    "st.markdown(\"<div class='input-box'><h3>Input Text:</h3></div>\", unsafe_allow_html=True)\n",
    "text_input = st.text_area(\"\", \"Type something...\", height=150)\n",
    "\n",
    "# Button to analyze sentiment\n",
    "if st.button(\"Analyze Sentiment\"):\n",
    "    if text_input.strip() == \"\":\n",
    "        st.error(\"Please enter some text before analyzing.\")\n",
    "    else:\n",
    "        # Clean and vectorize the input text\n",
    "        cleaned_text = clean_text(text_input)\n",
    "        vectorized_text = vectorizer.transform([cleaned_text]).toarray()\n",
    "\n",
    "        # Make a prediction\n",
    "        prediction = model.predict(vectorized_text)[0]\n",
    "        sentiment = \"Positive üòä\" if prediction == 1 else \"Negative üòî\"\n",
    "        sentiment_class = \"positive\" if prediction == 1 else \"negative\"\n",
    "\n",
    "        # Display the result\n",
    "        st.markdown(f\"\"\"\n",
    "            <div class='text-output {sentiment_class}'>\n",
    "                <h2 style='text-align:center;'>{sentiment}</h2>\n",
    "            </div>\n",
    "            <div class='main'>\n",
    "                <h3>Your Text:</h3>\n",
    "                <p>{text_input}</p>\n",
    "            </div>\n",
    "        \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "use all this codes and 1) create a detailed report on 'Sentiment Analysis on Social media using Machine learning' add user guide for end users. 2) create a readme file in format and short description for 3 lines to for my git hub repository . 3) write a post in linked in on 'Sentiment Analysis on Social media using Machine learning' on which im going to add my github repository  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
